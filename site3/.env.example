# Server Configuration
PORT=3000
NODE_ENV=development

# LLM Configuration
LLM_BASE_URL=http://your-llm-server:port/v1
# LLM_MODEL_ID=    # Leave blank to use already loaded model on the LM Studio server
# LLM_MODEL_ID=openai/gpt-oss-120b  # <-- General purpose model, doesn't work well with structured output
LLM_MODEL_ID=llama-3.1-8b-structuredie-v2.2 # Good for structured output

